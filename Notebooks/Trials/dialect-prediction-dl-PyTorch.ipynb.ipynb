{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-25T18:58:12.532346Z","iopub.status.busy":"2024-05-25T18:58:12.532029Z","iopub.status.idle":"2024-05-25T18:58:13.504918Z","shell.execute_reply":"2024-05-25T18:58:13.503674Z","shell.execute_reply.started":"2024-05-25T18:58:12.532320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'Arabic-Dialect-Classification' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/adelelwan24/Arabic-Dialect-Classification.git"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:50:47.376585Z","iopub.status.busy":"2024-05-26T09:50:47.375790Z","iopub.status.idle":"2024-05-26T09:50:59.867996Z","shell.execute_reply":"2024-05-26T09:50:59.866807Z","shell.execute_reply.started":"2024-05-26T09:50:47.376537Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["%%capture\n","!pip install transformers torch arabert"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:50:59.870393Z","iopub.status.busy":"2024-05-26T09:50:59.870058Z","iopub.status.idle":"2024-05-26T09:50:59.875823Z","shell.execute_reply":"2024-05-26T09:50:59.874917Z","shell.execute_reply.started":"2024-05-26T09:50:59.870330Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, classification_report"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:50:59.877230Z","iopub.status.busy":"2024-05-26T09:50:59.876953Z","iopub.status.idle":"2024-05-26T09:51:00.317292Z","shell.execute_reply":"2024-05-26T09:51:00.316343Z","shell.execute_reply.started":"2024-05-26T09:50:59.877207Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((118084, 2), (29533, 2))"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('/kaggle/working/Arabic-Dialect-Classification/cleaned data/cleaned_train.csv')\n","test  = pd.read_csv('/kaggle/working/Arabic-Dialect-Classification/cleaned data/cleaned_test.csv')\n","\n","train = train.astype(str)\n","test  = test.astype(str)\n","\n","train.shape, test.shape"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:00.320475Z","iopub.status.busy":"2024-05-26T09:51:00.320093Z","iopub.status.idle":"2024-05-26T09:51:00.448316Z","shell.execute_reply":"2024-05-26T09:51:00.446830Z","shell.execute_reply.started":"2024-05-26T09:51:00.320426Z"},"trusted":true},"outputs":[],"source":["train_EG = train[train['Dialect'] == 'EG'].sample(9224, random_state=2024)\n","train_LY = train[train['Dialect'] == 'LY'].sample(9224, random_state=2024)\n","train_LB = train[train['Dialect'] == 'LB'].sample(9224, random_state=2024)\n","train_SD = train[train['Dialect'] == 'SD'].sample(9224, random_state=2024)\n","train_MA = train[train['Dialect'] == 'MA'].sample(9224, random_state=2024)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:00.450400Z","iopub.status.busy":"2024-05-26T09:51:00.449949Z","iopub.status.idle":"2024-05-26T09:51:00.473594Z","shell.execute_reply":"2024-05-26T09:51:00.472662Z","shell.execute_reply.started":"2024-05-26T09:51:00.450320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(46120, 2)\n"]},{"data":{"text/plain":["Dialect\n","EG    9224\n","LY    9224\n","LB    9224\n","SD    9224\n","MA    9224\n","Name: count, dtype: int64"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.concat([train_EG, train_LY, train_LB,train_SD, train_MA])\n","print(df.shape)\n","df['Dialect'].value_counts()"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:00.474925Z","iopub.status.busy":"2024-05-26T09:51:00.474659Z","iopub.status.idle":"2024-05-26T09:51:00.482257Z","shell.execute_reply":"2024-05-26T09:51:00.481247Z","shell.execute_reply.started":"2024-05-26T09:51:00.474901Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["from arabert.preprocess import ArabertPreprocessor\n","\n","model_name=\"bert-base-arabertv02\"\n","arabert_prep = ArabertPreprocessor(model_name=model_name)\n","\n","text = \"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\n","arabert_prep.preprocess(text)\n","# >>>\"و+ لن نبالغ إذا قل +نا إن هاتف أو كمبيوتر ال+ مكتب في زمن +نا هذا ضروري\"\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:00.483787Z","iopub.status.busy":"2024-05-26T09:51:00.483444Z","iopub.status.idle":"2024-05-26T09:51:00.490096Z","shell.execute_reply":"2024-05-26T09:51:00.489244Z","shell.execute_reply.started":"2024-05-26T09:51:00.483760Z"},"trusted":true},"outputs":[],"source":["label_enc = LabelEncoder()"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:00.491637Z","iopub.status.busy":"2024-05-26T09:51:00.491247Z","iopub.status.idle":"2024-05-26T09:51:27.840034Z","shell.execute_reply":"2024-05-26T09:51:27.839238Z","shell.execute_reply.started":"2024-05-26T09:51:00.491599Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["46120 46120\n","Model Max Length: 512\n","Padding Token: [PAD]\n","Padding Token ID: 0\n","Vocabulary Size: 64000\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Sample Arabic data\n","texts = list(df['Text'].apply(arabert_prep.preprocess))\n","labels = list(label_enc.fit_transform(df['Dialect']))\n","print(len(texts), len(labels))\n","\n","assert len(texts) == len(labels), \"Each text must have a label.\"\n","\n","# Load pre-trained AraBERT tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02')\n","\n","# Print relevant attributes\n","print(f\"Model Max Length: {tokenizer.model_max_length}\")\n","print(f\"Padding Token: {tokenizer.pad_token}\")\n","print(f\"Padding Token ID: {tokenizer.pad_token_id}\")\n","print(f\"Vocabulary Size: {tokenizer.vocab_size}\")\n","\n","# Tokenize the data\n","inputs = tokenizer(texts, return_tensors='pt', max_length=tokenizer.model_max_length, \n","                   truncation=True, padding='max_length', return_token_type_ids=False)\n","\n","# print(inputs['input_ids'][:, :15])\n","# print(tokenizer.decode(inputs['input_ids'][0]))\n","\n","\n","# Create PyTorch dataset\n","dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n","\n","# Split dataset into training and validation sets\n","train_dataset, val_dataset = train_test_split(dataset, test_size=0.1)\n","\n","# Create DataLoader for training and validation sets\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n","val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=8)\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:27.841670Z","iopub.status.busy":"2024-05-26T09:51:27.841297Z","iopub.status.idle":"2024-05-26T09:51:27.848920Z","shell.execute_reply":"2024-05-26T09:51:27.847935Z","shell.execute_reply.started":"2024-05-26T09:51:27.841639Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['السلام', 'عليكم', '!', 'كيف', 'الحال', 'يا', 'شباب', '؟', '.', '.']"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.tokenize(arabert_prep.preprocess('السلام عليكم ! كيف الحال يا شباب؟ ..'))"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:27.853220Z","iopub.status.busy":"2024-05-26T09:51:27.852740Z","iopub.status.idle":"2024-05-26T09:51:27.859673Z","shell.execute_reply":"2024-05-26T09:51:27.858790Z","shell.execute_reply.started":"2024-05-26T09:51:27.853188Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["عامه الشعب : شوفوا الفلوس الشهره الحلاوه\n","['عامه', 'الشعب', ':', 'شوف', '##وا', 'الفلوس', 'الشهر', '##ه', 'الحل', '##او', '##ه']\n","--------------------\n","شريكه طبعا وقولتلك ' مباعه وبالقانون ، وشريكه بدايه بالسكوت وتعمل نايمه ومش جديد زمان . افتكر الترشح لنواب مش اتحاد ' سمعه اعضاءه سبقاه ! قياس بقي وتطبيق عالاقل اهميه نتوقع نتيجته ايه يعني ! ؟ نسكت احسن وبلاش نفتكر , الكلام مر . اشتري اعصابك\n","['شريكه', 'طبعا', 'وقول', '##تل', '##ك', \"'\", 'مبا', '##عه', 'وبال', '##قان', '##ون', '،', 'وشريك', '##ه', 'بدا', '##يه', 'بالس', '##كوت', 'وتعمل', 'نايم', '##ه', 'ومش', 'جديد', 'زمان', '.', 'افت', '##كر', 'الترشح', 'لنواب', 'مش', 'اتحاد', \"'\", 'سمعه', 'اعضاء', '##ه', 'سبق', '##اه', '!', 'قياس', 'بقي', 'وتطبيق', 'عال', '##اق', '##ل', 'اهم', '##يه', 'نتوقع', 'نتيجته', 'ايه', 'يعني', '!', '؟', 'نس', '##كت', 'احسن', 'وبلا', '##ش', 'نفت', '##كر', ',', 'الكلام', 'مر', '.', 'اشت', '##ري', 'اع', '##صاب', '##ك']\n","--------------------\n","ده مفيش اسرع الايام\n","['ده', 'مفيش', 'اسرع', 'الايام']\n","--------------------\n","اسمع ياود عميعلشان يخرج الكتكوت البيضهلازم يكسرهامش كسر تزعل عليهفيه حاجات لازم تكسرهاوبنفسكعشان تقب عالدنيا\n","['اسمع', 'يا', '##ود', 'عمي', '##عل', '##شان', 'يخرج', 'الكت', '##كوت', 'البيض', '##هلا', '##زم', 'يكسر', '##هام', '##ش', 'كسر', 'تز', '##عل', 'عليه', '##فيه', 'حاجات', 'لازم', 'تكسر', '##ها', '##وب', '##نفس', '##ك', '##عش', '##ان', 'تق', '##ب', 'عال', '##دن', '##يا']\n","--------------------\n"]}],"source":["for idx in range(4):\n","    print(texts[idx])\n","    print(tokenizer.tokenize(texts[idx]))\n","    print(\"-\" * 20)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:27.861219Z","iopub.status.busy":"2024-05-26T09:51:27.860878Z","iopub.status.idle":"2024-05-26T09:51:27.912579Z","shell.execute_reply":"2024-05-26T09:51:27.911519Z","shell.execute_reply.started":"2024-05-26T09:51:27.861189Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary Size: 64000\n","Token: المتش, Token ID: 25311\n","Token: لشبونة, Token ID: 26535\n","Token: توزيعات, Token ID: 47028\n","Token: تجعلهم, Token ID: 49514\n","Token: [UNUSED_1290], Token ID: 61290\n","Token: وامكان, Token ID: 54964\n","Token: [UNUSED_1378], Token ID: 61378\n","Token: تزال, Token ID: 4038\n","Token: الشامي, Token ID: 24976\n","Token: تزوجت, Token ID: 23161\n"]}],"source":["# Get the vocabulary\n","vocab = tokenizer.get_vocab()\n","\n","# Print the size of the vocabulary\n","print(f\"Vocabulary Size: {len(vocab)}\")\n","\n","# Print a sample of the vocabulary (first 20 items)\n","for token, token_id in list(vocab.items())[2000:2010]:\n","    print(f\"Token: {token}, Token ID: {token_id}\")"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:27.914098Z","iopub.status.busy":"2024-05-26T09:51:27.913784Z","iopub.status.idle":"2024-05-26T09:51:27.927632Z","shell.execute_reply":"2024-05-26T09:51:27.926829Z","shell.execute_reply.started":"2024-05-26T09:51:27.914071Z"},"trusted":true},"outputs":[],"source":["tokens = vocab.keys()"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:27.929167Z","iopub.status.busy":"2024-05-26T09:51:27.928816Z","iopub.status.idle":"2024-05-26T09:51:27.940831Z","shell.execute_reply":"2024-05-26T09:51:27.939852Z","shell.execute_reply.started":"2024-05-26T09:51:27.929135Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["!\t?\t|\t\"\t@\t{\t-\t]\t=\t\\\t^\t<\t.\t%\t#\t`\t,\t+\t$\t'\t}\t;\t*\t(\t~\t[\t_\t)\t>\t:\t"]}],"source":["#### Assert that Punctuation exist in the vocab\n","for token in tokens:\n","    if token in \"\"\" !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~. \"\"\":\n","        print(token, end='\\t')"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:27.942501Z","iopub.status.busy":"2024-05-26T09:51:27.942139Z","iopub.status.idle":"2024-05-26T09:51:27.954982Z","shell.execute_reply":"2024-05-26T09:51:27.954101Z","shell.execute_reply.started":"2024-05-26T09:51:27.942450Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["a\th\td\tj\tq\tg\tt\ti\tr\tz\tn\tm\ts\tp\tl\te\tk\tf\ty\tw\to\tv\tb\tu\tc\tx\t"]}],"source":["#### Assert that English Letters exist in the vocab\n","for token in tokens:\n","    if token in \"\"\" qwertyuiopasdfghjklzxcvbnm \"\"\":\n","        print(token, end='\\t')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T07:21:37.576095Z","iopub.status.busy":"2024-05-26T07:21:37.575690Z","iopub.status.idle":"2024-05-26T07:21:40.546762Z","shell.execute_reply":"2024-05-26T07:21:40.545809Z","shell.execute_reply.started":"2024-05-26T07:21:37.576063Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c26e1e3d25cc4e419779daf638bc20fb","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Load pre-trained AraBERT model for sequence classification\n","model = AutoModelForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabertv02', num_labels=len(label_enc.classes_))\n","\n","# Move model to GPU if available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T07:11:38.468661Z","iopub.status.busy":"2024-05-26T07:11:38.467944Z","iopub.status.idle":"2024-05-26T07:11:38.472710Z","shell.execute_reply":"2024-05-26T07:11:38.471799Z","shell.execute_reply.started":"2024-05-26T07:11:38.468629Z"},"trusted":true},"outputs":[],"source":["# # Load pre-trained AraBERT model for sequence classification\n","# model = AutoModelForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabertv02', num_labels=len(label_enc.classes_))\n","\n","# # Move model to GPUs\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model.to(device)\n","\n","# # Enable multi-GPU training\n","# if torch.cuda.device_count() > 1:\n","#     print(f'Cuda count : {torch.cuda.device_count()}')\n","#     model = torch.nn.DataParallel(model)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T07:11:40.484839Z","iopub.status.busy":"2024-05-26T07:11:40.484133Z","iopub.status.idle":"2024-05-26T07:11:40.490567Z","shell.execute_reply":"2024-05-26T07:11:40.489633Z","shell.execute_reply.started":"2024-05-26T07:11:40.484806Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Max Length: 512\n","Number of Labels: 5\n","Hidden Size: 768\n","Vocab Size: 64000\n","Type Vocabulary Size: 2\n"]}],"source":["# Access the model configuration\n","model_config = model.config\n","\n","# Print relevant attributes\n","print(f\"Model Max Length: {model_config.max_position_embeddings}\")\n","print(f\"Number of Labels: {model_config.num_labels}\")\n","print(f\"Hidden Size: {model_config.hidden_size}\")\n","print(f\"Vocab Size: {model_config.vocab_size}\")\n","print(f\"Type Vocabulary Size: {model_config.type_vocab_size}\")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:20:54.722911Z","iopub.status.busy":"2024-05-25T20:20:54.722487Z","iopub.status.idle":"2024-05-25T23:31:56.858380Z","shell.execute_reply":"2024-05-25T23:31:56.857325Z","shell.execute_reply.started":"2024-05-25T20:20:54.722876Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","Validation Loss: 0.6546125508573688\n","Validation Accuracy: 0.7549869904596704\n","Validation F1 Score: 0.7531040379663219\n","Model saved to arabert_model_epoch_1\n","Epoch 2/3\n","Validation Loss: 0.6301433282098903\n","Validation Accuracy: 0.769297484822203\n","Validation F1 Score: 0.7699146961751101\n","Model saved to arabert_model_epoch_2\n","Epoch 3/3\n","Validation Loss: 0.62845909396958\n","Validation Accuracy: 0.7862098872506504\n","Validation F1 Score: 0.7855138673981219\n","Model saved to arabert_model_epoch_3\n"]}],"source":["from torch.optim import AdamW\n","\n","# Define optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# Training loop\n","epochs = 3\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        \n","        # Clear previously calculated gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        outputs = model(**inputs)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","        \n","        # Backward pass\n","        loss.backward()\n","        \n","        # Update weights\n","        optimizer.step()\n","        \n","    \n","    # Validation\n","    model.eval()\n","    eval_loss = 0\n","    eval_steps = 0\n","    predictions, true_labels = [], []\n","    \n","    for batch in val_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        \n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","            \n","        eval_loss += loss.item()\n","        eval_steps += 1\n","        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","        true_labels.extend(inputs['labels'].cpu().numpy())\n","    \n","    avg_eval_loss = eval_loss / eval_steps\n","    accuracy = accuracy_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions, average='macro')\n","    \n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","    print(f\"Validation Loss: {avg_eval_loss}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(f\"Validation F1 Score: {f1}\")\n","    \n","    # Save the model\n","    model_save_path = f\"arabert_model_epoch_{epoch + 1}\"\n","    model.save_pretrained(model_save_path)\n","    tokenizer.save_pretrained(model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T08:30:20.411313Z","iopub.status.busy":"2024-05-26T08:30:20.410392Z","iopub.status.idle":"2024-05-26T08:30:26.138583Z","shell.execute_reply":"2024-05-26T08:30:26.137798Z","shell.execute_reply.started":"2024-05-26T08:30:20.411272Z"},"trusted":true},"outputs":[],"source":["# Example Arabic test data\n","test_texts = list(test['Text'])\n","test_labels = list(label_enc.transform(test['Dialect']))\n","\n","# Tokenize test data\n","test_inputs = tokenizer(test_texts, return_tensors='pt', max_length=128, truncation=True, padding='max_length')\n","test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], torch.tensor(test_labels))\n","test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)\n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T23:31:56.860358Z","iopub.status.busy":"2024-05-25T23:31:56.859968Z","iopub.status.idle":"2024-05-25T23:35:29.817113Z","shell.execute_reply":"2024-05-25T23:35:29.816120Z","shell.execute_reply.started":"2024-05-25T23:31:56.860320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8021535231774625\n","Classification Report: \n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.82      0.86     11525\n","           1       0.73      0.90      0.81      5523\n","           2       0.84      0.75      0.79      7296\n","           3       0.70      0.77      0.74      2308\n","           4       0.65      0.69      0.67      2881\n","\n","    accuracy                           0.80     29533\n","   macro avg       0.76      0.79      0.77     29533\n","weighted avg       0.81      0.80      0.80     29533\n","\n"]}],"source":["# Evaluate the model\n","model.eval()\n","test_predictions, test_true_labels = [], []\n","\n","for batch in test_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","    \n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","    \n","    test_predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","    test_true_labels.extend(inputs['labels'].cpu().numpy())"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T23:53:08.940158Z","iopub.status.busy":"2024-05-25T23:53:08.939417Z","iopub.status.idle":"2024-05-25T23:53:08.994641Z","shell.execute_reply":"2024-05-25T23:53:08.993770Z","shell.execute_reply.started":"2024-05-25T23:53:08.940123Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8021535231774625\n","Test F1 Score: 0.7855138673981219\n","Classification Report: \n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.82      0.86     11525\n","           1       0.73      0.90      0.81      5523\n","           2       0.84      0.75      0.79      7296\n","           3       0.70      0.77      0.74      2308\n","           4       0.65      0.69      0.67      2881\n","\n","    accuracy                           0.80     29533\n","   macro avg       0.76      0.79      0.77     29533\n","weighted avg       0.81      0.80      0.80     29533\n","\n"]}],"source":["test_accuracy = accuracy_score(test_true_labels, test_predictions)\n","class_report = classification_report(test_true_labels, test_predictions)\n","f1 = f1_score(true_labels, predictions, average='macro')\n","\n","\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Classification Report: \\n{class_report}\")"]},{"cell_type":"markdown","metadata":{},"source":["-----"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:41.984919Z","iopub.status.busy":"2024-05-26T09:51:41.984558Z","iopub.status.idle":"2024-05-26T09:51:42.408374Z","shell.execute_reply":"2024-05-26T09:51:42.407576Z","shell.execute_reply.started":"2024-05-26T09:51:41.984890Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# Load pre-trained AraBERT model for sequence classification\n","model_loaded = AutoModelForSequenceClassification.from_pretrained('/kaggle/working/arabert_model_fine_tuned', num_labels=len(label_enc.classes_))\n","\n","# Move model to GPU if available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model_loaded.to(device)\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:51:45.186638Z","iopub.status.busy":"2024-05-26T09:51:45.185762Z","iopub.status.idle":"2024-05-26T09:55:29.614810Z","shell.execute_reply":"2024-05-26T09:55:29.613734Z","shell.execute_reply.started":"2024-05-26T09:51:45.186601Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model\n","model_loaded.eval()\n","test_predictions, test_true_labels = [], []\n","\n","for batch in test_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","    \n","    with torch.no_grad():\n","        outputs = model_loaded(**inputs)\n","        logits = outputs.logits\n","    \n","    test_predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","    test_true_labels.extend(inputs['labels'].cpu().numpy())"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:55:29.617158Z","iopub.status.busy":"2024-05-26T09:55:29.616793Z","iopub.status.idle":"2024-05-26T09:55:29.676341Z","shell.execute_reply":"2024-05-26T09:55:29.675236Z","shell.execute_reply.started":"2024-05-26T09:55:29.617128Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8021535231774625\n","Test F1 Score: 0.9248752183643715\n","Classification Report: \n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.82      0.86     11525\n","           1       0.73      0.90      0.81      5523\n","           2       0.84      0.75      0.79      7296\n","           3       0.70      0.77      0.74      2308\n","           4       0.65      0.69      0.67      2881\n","\n","    accuracy                           0.80     29533\n","   macro avg       0.76      0.79      0.77     29533\n","weighted avg       0.81      0.80      0.80     29533\n","\n"]}],"source":["test_accuracy = accuracy_score(test_true_labels, test_predictions)\n","class_report = classification_report(test_true_labels, test_predictions)\n","f1 = f1_score(test_true_labels, test_predictions, average='macro')\n","\n","\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Classification Report: \\n{class_report}\")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T09:55:29.678442Z","iopub.status.busy":"2024-05-26T09:55:29.678100Z","iopub.status.idle":"2024-05-26T10:58:09.919316Z","shell.execute_reply":"2024-05-26T10:58:09.918281Z","shell.execute_reply.started":"2024-05-26T09:55:29.678411Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","Validation Loss: 0.2531024248494994\n","Validation Accuracy: 0.9180398959236774\n","Validation F1 Score: 0.9176213796319722\n","Model saved to arabert_model_fine_tuned_1\n"]}],"source":["from torch.optim import AdamW\n","\n","# Define optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# Training loop\n","epochs = 1\n","\n","for epoch in range(epochs):\n","    model_loaded.train()\n","    for batch in train_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        \n","        # Clear previously calculated gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        outputs = model_loaded(**inputs)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","        \n","        # Backward pass\n","        loss.backward()\n","        \n","        # Update weights\n","        optimizer.step()\n","        \n","    \n","    # Validation\n","    model_loaded.eval()\n","    eval_loss = 0\n","    eval_steps = 0\n","    predictions, true_labels = [], []\n","    \n","    for batch in val_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","        \n","        with torch.no_grad():\n","            outputs = model_loaded(**inputs)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","            \n","        eval_loss += loss.item()\n","        eval_steps += 1\n","        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","        true_labels.extend(inputs['labels'].cpu().numpy())\n","    \n","    avg_eval_loss = eval_loss / eval_steps\n","    accuracy = accuracy_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions, average='macro')\n","    \n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","    print(f\"Validation Loss: {avg_eval_loss}\")\n","    print(f\"Validation Accuracy: {accuracy}\")\n","    print(f\"Validation F1 Score: {f1}\")\n","    \n","    # Save the model\n","    model_save_path = f\"arabert_model_fine_tuned_{epoch + 1}\"\n","    model_loaded.save_pretrained(model_save_path)\n","    tokenizer.save_pretrained(model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:58:09.922162Z","iopub.status.busy":"2024-05-26T10:58:09.921788Z","iopub.status.idle":"2024-05-26T11:01:52.055572Z","shell.execute_reply":"2024-05-26T11:01:52.054727Z","shell.execute_reply.started":"2024-05-26T10:58:09.922128Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model\n","model_loaded.eval()\n","test_predictions, test_true_labels = [], []\n","\n","for batch in test_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","    \n","    with torch.no_grad():\n","        outputs = model_loaded(**inputs)\n","        logits = outputs.logits\n","    \n","    test_predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","    test_true_labels.extend(inputs['labels'].cpu().numpy())"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T11:20:15.090259Z","iopub.status.busy":"2024-05-26T11:20:15.089406Z","iopub.status.idle":"2024-05-26T11:20:15.167940Z","shell.execute_reply":"2024-05-26T11:20:15.167136Z","shell.execute_reply.started":"2024-05-26T11:20:15.090228Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.8021535231774625\n","Test F1 Score: 0.7714423177358425\n","Classification Report: \n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.82      0.86     11525\n","           1       0.73      0.90      0.81      5523\n","           2       0.84      0.75      0.79      7296\n","           3       0.70      0.77      0.74      2308\n","           4       0.65      0.69      0.67      2881\n","\n","    accuracy                           0.80     29533\n","   macro avg       0.76      0.79      0.77     29533\n","weighted avg       0.81      0.80      0.80     29533\n","\n"]}],"source":["test_accuracy = accuracy_score(test_true_labels, test_predictions)\n","class_report = classification_report(test_true_labels, test_predictions)\n","f1 = f1_score(test_true_labels, test_predictions, average='macro')\n","\n","\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test F1 Score: {f1}\")\n","print(f\"Classification Report: \\n{class_report}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T08:43:42.731100Z","iopub.status.busy":"2024-05-26T08:43:42.730713Z","iopub.status.idle":"2024-05-26T08:44:12.298273Z","shell.execute_reply":"2024-05-26T08:44:12.297159Z","shell.execute_reply.started":"2024-05-26T08:43:42.731058Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/arabert_model_fine_tuned/ (stored 0%)\n","  adding: kaggle/working/arabert_model_fine_tuned/special_tokens_map.json (deflated 80%)\n","  adding: kaggle/working/arabert_model_fine_tuned/tokenizer_config.json (deflated 79%)\n","  adding: kaggle/working/arabert_model_fine_tuned/vocab.txt (deflated 65%)\n","  adding: kaggle/working/arabert_model_fine_tuned/tokenizer.json (deflated 74%)\n","  adding: kaggle/working/arabert_model_fine_tuned/config.json (deflated 53%)\n","  adding: kaggle/working/arabert_model_fine_tuned/model.safetensors (deflated 7%)\n"]}],"source":["!zip -r /kaggle/working/arabert_model_fine_tuned.zip  /kaggle/working/arabert_model_fine_tuned"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T08:44:41.625220Z","iopub.status.busy":"2024-05-26T08:44:41.624810Z","iopub.status.idle":"2024-05-26T08:44:41.632368Z","shell.execute_reply":"2024-05-26T08:44:41.631525Z","shell.execute_reply.started":"2024-05-26T08:44:41.625181Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='arabert_model_fine_tuned.zip' target='_blank'>arabert_model_fine_tuned.zip</a><br>"],"text/plain":["/kaggle/working/arabert_model_fine_tuned.zip"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink \n","# FileLink(r'arabert_classification.zip')\n","FileLink(r'arabert_model_fine_tuned.zip')\n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
